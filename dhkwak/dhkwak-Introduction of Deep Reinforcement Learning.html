<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    line-height:35px;background-color: black;color:#ABBAB7;
}
img {
 width:900px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"],
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
dhkwak-Introduction of Deep Reinforcement Learning.html
<xmp>
# https://www.youtube.com/watch?v=dw0sHzE1oAc&t=378s

# Reinforcement learning:
# Goal is to find policy function p(a|s) which maximizes sum of reward

# s: current state where agent which you train is in
# a: in that state, what action agent should do?

# @
# When you want to find policy,
# you need cycle composed of "agent", "environment", "interaction"(state, reward, action) as tool
# img 2018-04-29 18-54-18.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 18-54-18.png"><xmp>
    
# To define cycle, you need mathematical tool (MDP: Markov Decision Processes)

# You will see MDP in 3 steps 
# (Markov process, Markov Reward Process, Markov Decision Process)

# @
# Markov Process is defined by two factors

# 1. Discrete state space (like circles)
# Suppose 5 by 5 maze
# Mice can be located in 25 locations
# In this case, size of state space is 25

# 1. State transition probability:
# This is process of finding rule from environment
# This is dynamics about environment

# img 2018-04-29 19-03-44.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 19-03-44.png"><xmp>

# @
# Markov Reward Processes
# This is concept added by "reward" (+1, -1) on "Markov Processes"
# If agent arrives to A, it gets +1 reward
# MRP lets agent to know state A is better state
# img 2018-04-29 19-06-54.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 19-06-54.png"><xmp>

# @
# Markov Decision Processes
# This is added by "action" 
# In this example,
# Discrete state space is S={A,B}
# Discrete action space is A={X,Y}
# (Action conditional) State transition probability:
# $$$P(S'|S,A)={...}$$$
# Reward function:
# $$$R(S'=A)=+1, R(S'=B)=-1$$$ 

# At state of A, which action will give agent more reward?
# Finding policy for it is goal of reinforcement learning

# At state A, agent has 2 options (action X, action Y)
# Since moving to A gives + reward, moving to B is false action
# So, if agent is in A, it should stay in A
# So, at state of A, opmimal decision is to select action Y
# By same logic, at state of B, 
# agent should make decision (Y in this case) to move to state of A
# img 2018-04-29 19-37-40.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 19-37-40.png"><xmp>

# @
# Summary: MDP is mathematical framework for modeling decision making

# @
# MDP question can be solved by two methods (planning, reinforcement learning)

# Planning: Exhaustive search/Dynamic programming
# Reinforcement Learning: MC method/TD learning(Q learning)

# Dynamic programming is not machine learning methodology,
# but similar to reinforcement learning in terms of mathematical way.

# img 2018-04-29 19-44-28.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 19-44-28.png"><xmp>

# img 2018-04-29 19-48-36.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 19-48-36.png"><xmp>

# Above agent considers entire reward up to future cases,
# which means it can eat -1 reward to get +3 reward

# Reinforcement learning wants to deal with "sparse reward" problem and "delayed reward" problem

# @
# You have 2 methodologies (Planning, RL) to find expectation value
# Planning:
# You already know probability of occurring each side from dice is $$$\frac{1}{6}$$$
# In this case, you can find expection value (3.5) easily.
# $$$1*\frac{1}{6}+2*\frac{1}{6}+...+6*\frac{1}{6}=3.5$$$

# RL:
# When you don't know probability of occurring each side,
# you should find expectation value from manual tries
# You should throw dices like 100 times,
# then you find mean value of dice side

# @
# You should consider "discount factor" to complete Markov Decision Process model
# Discount factor is concept which was created to mathematically define "future reward"

# Sum of future rewards in "episodic tasks"

# Episodic task: task which has end point (T), for example, go match
# t: arbitary current time point
# R: reward
# $$$G_{t}=R_{t+1}+R_{t+2}+...+R_{T}$$$


# Continuous tasks: task which has no end point (T), for example, car driving, stock market
# Sum of "future rewards" in continuous tasks
# $$$G_{t}=R_{t+1}+R_{t+2}+...+R_{T}+...$$$
# $$$G_{t}$$$ can be diverged
# $$$G_{t} \rightarrow \infty$$$


# To mathematically manage above cases,
# you multiply "discount factor" ($$$0<= \gamma<1$$$)
# Sum of discounted "future rewards" in both case
# $$$G_{t}:=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+...+\gamma^{T-1}R_{T}+...$$$
# $$$G_{t}:= \sum\limits_{k=1}^{\infty} \gamma^{k-1}R_{t+k} $$$ (This will be converged)

# There are 2 kinds of policy,
# which is object of train,
# and is function that lets agent to know what action he should do
# deterministic policy, stochastic policy
# img 2018-04-29 20-37-32.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 20-37-32.png"><xmp>
# img 2018-04-29 20-37-59.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 20-37-59.png"><xmp>

# Some cases are fitted to stochastic policy,
# and some other cases are fitted to deterministic policy

# @
# You will see value based reinforment learning
# Value is expection of sum of future rewards
# You can write sum of future rewards in episodic tasks as following:
# $$$G_{t}:=R_{t+1}+R_{t+2}+...+R_{T}$$$
# You can write sum of future rewards in continuous tasks as following:
# $$$G_{t}:=R_{t+1}+R_{t+2}+...+R_{T}+...$$$
# $$$G_{t}\rightarrow \infty$$$ ($$$G_{t}$$$ diverges)

# You can write sum of discounted future rewards in both cases (episodic and continuous) as following
# $$$G_{t}:=R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+...+\gamma^{T-1}R_{T}+...$$$
# $$$G_{t}:=\sum\limits_{k=1}^{\infty}\gamma^{k-1}R_{t+k}$$$
# This version of $$$G_{t}$$$ converges

# b) is values at each state
# c) is optimal policy
# img 2018-04-29 20-54-19.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 20-54-19.png"><xmp>

# @
# "Value function" lets you know expected sum of "future rewards" 
# at given state s, following policy $$$\pi$$$
# value function takes in state as input, outputs value
# 1. State-value function
# $$$v_{\pi}(s)\doteq E_{\pi}[G_{t}|S_{t}=s]$$$
# $$$v_{\pi}(s)\doteq E_{\pi}[\sum\limits_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_{t}=s]$$$
# If you use only state-value function,
# to make decision, you should need exploration
# You should simulate all actions you can use,
# to inspect next state's values
# This method is called "one step ahead search" for all actions
# In real world situation, one step ahead search is not possible to use

# 1. (State)-Action-value function
# $$$q_{\pi}(s,a) \doteq E_{\pi}[S_{t}-s,A_{t}=a]$$$
# $$$v_{\pi}(s) \doteq E_{\pi}[\sum\limits_{k=0}^{\infty}\gamma^{k}R_{t+k+1}|S_{t}=s,A_{t}=a$$$
# This complements state-value fucntion
# This doesn't require one step ahead search because it's already processed in advance
# Note that you input s and a into q function

# img 2018-04-29 21-04-38.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 21-04-38.png"><xmp>

# @
# When you find expection value,
# if you do it with probability, it's planning,
# if you do it without probability but experience, it's machine/deep/reinforment learing

# img 2018-04-29 21-06-41.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 21-06-41.png"><xmp>

# @
# One of solutions for MDP: Planning
# img 2018-04-29 21-08-11.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 21-08-11.png"><xmp>

# @
# N: number of state
# img 2018-04-29 21-10-57.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 21-10-57.png"><xmp>


# @
# Policy iteration consists of Policy evaluation and policy improvement

# Policy evaluation:
# By using knowledge you know, you make current value function more precise.
# $$$v_{\pi}(s)\doteq E_{\pi}[R_{t+1}+\gamma R_{t+2}+\gamma^{2}R_{t+3}+...|S_{t}=s]$$$
# $$$v_{\pi}(s)\doteq E_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s]$$$
# $$$v_{\pi}(s)=\sum\limits_{a}\pi(a|s)\sum\limits_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')]$$$

# Policy improvement:
# By using more precisevalue function, you try to make more optimal decision
# $$$\pi'(s)\doteq arg_{a}max\;q_{\pi}(s,a)$$$
# $$$\pi'(s)\doteq arg_{a}max\;E[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_{t}=s,A_{t}=a]$$$
# $$$\pi'(s)\doteq arg_{a}max\;\sum\limits_{s',r}p(s',r|s,a)[r+\gamma v_{\pi}(s')]$$$

# Then, finally you can find optimal value function and policy

# @
# To reduce number of cases, you can use dynamic programming
# img 2018-04-29 21-11-56.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/dhkwak/pic/2018-04-29 21-11-56.png"><xmp>


# @
# Temporal Difference Learning
# This shows good performance
# Good point of Monte Carlo is Monte Carlo can train from experience
# Good point of dynamic programming is dynamic programming reduces quantity of calculation
# because dynamic programming only calculates in respect to 2 time steps
# TD is combined algorithm from above 2 good points


# @
# Difference between sarsa and q learing
# Formulars are almost same.
# This is sarsa notation
# $$$Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha [r_{t+1}+\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})]$$$
# $$$[r_{t+1}+\gamma Q(s_{t+1},a_{t+1}) - Q(s_{t},a_{t})]$$$ is loss of q function you're having now
# You need 5 variables as input data ($$$s_{t}, a_{t}, r_{t+1}, s_{t+1}, a_{t+1}$$$) to create above notation

# What's good point of sarsa algorithm?
# If you use Monte Carlo method,
# you should proceed task up to end of episode,
# then, you get values, then you apply obtained values to all state to update

# If you use temporal difference learing,
# you can update by very next reward every step

# Both method can learn perfect value function with different method

# Q learning has "max" operator
# Sarsa is not good because it doesn't have "max" operator
# $$$Q(s_{t},a_{t}) \leftarrow Q(s_{t},a_{t}) + \alpha [r_{t+1}+\gamma max_{a} Q(s_{t+1},a) - Q(s_{t},a_{t})]$$$
# You need 4 variables as input data (s_{t}, a_{t}, r_{t+1}, s_{t+1}) to create above notation
# Reason you need only 4 variables is you use max operator in respect to general a
# $$$ [r_{t+1}+\gamma max_{a} Q(s_{t+1},a) - Q(s_{t},a_{t})] $$$ is TD loss
# You use size of TD loss as loss, then you train with regression way
# $$$\text{Loss function L}=\frac{1}{2}[r+max_{a'}Q(s',a')-Q(s,a)]^{2}$$$
# $$$r+max_{a'}Q(s',a')$$$ is target
# Q(s,a) is prediction from model

# <
# Above difference can also be notated by difference between on policy and off policy
# on policy : sarsa is on policy.
# Sarsa only can be used 
# when target policy which you need to train is same 
# with behavior policy which you're using 

# off policy: 
# It's fine with both cases that same or not same
# off policy is q learning
# >

# @
# Unstationary target issue


# @
# Replay memory

</xmp>
   </BODY>
</HTML>
