<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 20px;
 
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 100px;
    
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    
    line-height:35px;
}
img {
 width:900px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">    
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"], 
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
<xmp>
https://www.youtube.com/watch?v=Fcmgl8ow2Uc&list=PLlMkM4tgfjnKsCWav-Z2F-MMFRx-2gMGG&index=13

================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/pracrl/master/shkim-rl/pic/2019_04_25_08:37:02.png' alt=''><xmp>

State as input is from 0 to 15

================================================================================
You can use one hot representation

</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/pracrl/master/shkim-rl/pic/2019_04_25_08:38:09.png' alt=''><xmp>

================================================================================
* 16 number of states (from 0 to 15)

* (16,16) 2D array is needed

* You can use np.identify()

* np.identity(16)
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/pracrl/master/shkim-rl/pic/2019_04_25_08:40:26.png' alt=''><xmp>

* State 0: np.identify(16)[0:0+1]
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/pracrl/master/shkim-rl/pic/2019_04_25_08:41:16.png' alt=''><xmp>

================================================================================
</xmp><img src='https://raw.githubusercontent.com/youngminpark2559/pracrl/master/shkim-rl/pic/2019_04_25_08:43:54.png' alt=''><xmp>

Input: (1,16) 2D array for one state
Output: (4,) 1D array for 4 actions

================================================================================
</xmp>
<!--
import gym
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

# ================================================================================
# c env: stochastic (non-deterministic) environment
env=gym.make('FrozenLake-v0')

# ================================================================================
input_size=env.observation_space.n
output_size=env.action_space.n

# ================================================================================
learning_rate=0.1

# ================================================================================
# c X: placeholder for input
X=tf.placeholder(shape=[1,input_size],dtype=tf.float32)

# c W: Variable for trainable weight
# 0,0.01 are for initialization
W=tf.Variable(tf.random_uniform([input_size,output_size],0,0.01))

# ================================================================================
# c Qpred: is q values from $$$\hat{Q}$$$ function, 
# representing predictied probabilities for each action
Qpred=tf.matmul(X,W)

# ================================================================================
# c Y: is output representing each action as one hot vector
Y=tf.placeholder(shape=[1,output_size],dtype=tf.float32)

# ================================================================================
# Since it's matrix, you should use reduce_sum
loss=tf.reduce_sum(tf.square(Y-Qpred))
train=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)

# ================================================================================
dis=.99
num_episodes=2000

# ================================================================================
# You create list to save history of summed reward per episode
rList=[]

# ================================================================================
def one_hot(x):
  return np.identity(16)[x:x+1]

# ================================================================================
init=tf.global_variables_initializer()

# ================================================================================
with tf.Session() as sess:
  sess.run(init)

  ================================================================================
  for i in range(num_episodes):
    s=env.reset() # Reset env per episode
    e=1./((i/50)+10) # For exploration
    rAll=0
    done=False
    local_loss=[]

    ================================================================================
    while not done: # Q-network training

      Qs=sess.run(Qpred,feed_dict={X:one_hot(s)})

      if np.random.rand(1)<e: # Exploration
        a=env.action_space.sample()
      else:                   # Exploitation
        a=np.argmax(Qs)

      # ================================================================================
      # Execute action and get data from env
      s1,reward,done,_=env.step(a)

      ================================================================================
      if done: # If episode ended
        # you update reward in Q value
        Qs[0,a]=reward

      else:    # If episode not ended
        # Q value at next state
        Qs1=sess.run(Qpred,feed_dict={X:one_hot(s1)})
        Qs[0,a]=reward+dis*np.max(Qs1)

      # ================================================================================
      # You train your network by using target Y and X (state)
      sess.run(train,feed_dict={X:one_hot(s),Y:Qs})

      # ================================================================================
      rAll+=reward
      s=s1
    
    # ================================================================================
    rList.append(rAll)

# ================================================================================
print("Percent of successful episodes: "+str(sum(rList)/num_episodes)+"%")
plt.bar(range(len(rList)),rList,color="blue")
plt.show()
-->

<!--
================================================================================
-->

<!-- HTML generated using hilite.me -->
<div style="background: #ffffff; overflow:auto;width:auto;border:solid gray;border-width:.1em .1em .1em .8em;padding:.2em .6em;"><pre style="margin: 0; line-height: 125%"><span style="color: #008800; font-weight: bold">import</span> <span style="color: #0e84b5; font-weight: bold">gym</span>
<span style="color: #008800; font-weight: bold">import</span> <span style="color: #0e84b5; font-weight: bold">numpy</span> <span style="color: #008800; font-weight: bold">as</span> <span style="color: #0e84b5; font-weight: bold">np</span>
<span style="color: #008800; font-weight: bold">import</span> <span style="color: #0e84b5; font-weight: bold">tensorflow</span> <span style="color: #008800; font-weight: bold">as</span> <span style="color: #0e84b5; font-weight: bold">tf</span>
<span style="color: #008800; font-weight: bold">import</span> <span style="color: #0e84b5; font-weight: bold">matplotlib.pyplot</span> <span style="color: #008800; font-weight: bold">as</span> <span style="color: #0e84b5; font-weight: bold">plt</span>

<span style="color: #888888"># ================================================================================</span>
<span style="color: #888888"># c env: stochastic (non-deterministic) environment</span>
env<span style="color: #333333">=</span>gym<span style="color: #333333">.</span>make(<span style="background-color: #fff0f0">&#39;FrozenLake-v0&#39;</span>)

<span style="color: #888888"># ================================================================================</span>
input_size<span style="color: #333333">=</span>env<span style="color: #333333">.</span>observation_space<span style="color: #333333">.</span>n
output_size<span style="color: #333333">=</span>env<span style="color: #333333">.</span>action_space<span style="color: #333333">.</span>n

<span style="color: #888888"># ================================================================================</span>
learning_rate<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">0.1</span>

<span style="color: #888888"># ================================================================================</span>
<span style="color: #888888"># c X: placeholder for input</span>
X<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>placeholder(shape<span style="color: #333333">=</span>[<span style="color: #0000DD; font-weight: bold">1</span>,input_size],dtype<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>float32)

<span style="color: #888888"># c W: Variable for trainable weight</span>
<span style="color: #888888"># 0,0.01 are for initialization</span>
W<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>Variable(tf<span style="color: #333333">.</span>random_uniform([input_size,output_size],<span style="color: #0000DD; font-weight: bold">0</span>,<span style="color: #6600EE; font-weight: bold">0.01</span>))

<span style="color: #888888"># ================================================================================</span>
<span style="color: #888888"># c Qpred: is q values from $$$\hat{Q}$$$ function, </span>
<span style="color: #888888"># representing predictied probabilities for each action</span>
Qpred<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>matmul(X,W)

<span style="color: #888888"># ================================================================================</span>
<span style="color: #888888"># c Y: is output representing each action as one hot vector</span>
Y<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>placeholder(shape<span style="color: #333333">=</span>[<span style="color: #0000DD; font-weight: bold">1</span>,output_size],dtype<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>float32)

<span style="color: #888888"># ================================================================================</span>
<span style="color: #888888"># Since it&#39;s matrix, you should use reduce_sum</span>
loss<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>reduce_sum(tf<span style="color: #333333">.</span>square(Y<span style="color: #333333">-</span>Qpred))
train<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>train<span style="color: #333333">.</span>GradientDescentOptimizer(learning_rate<span style="color: #333333">=</span>learning_rate)<span style="color: #333333">.</span>minimize(loss)

<span style="color: #888888"># ================================================================================</span>
dis<span style="color: #333333">=.</span><span style="color: #0000DD; font-weight: bold">99</span>
num_episodes<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">2000</span>

<span style="color: #888888"># ================================================================================</span>
<span style="color: #888888"># You create list to save history of summed reward per episode</span>
rList<span style="color: #333333">=</span>[]

<span style="color: #888888"># ================================================================================</span>
<span style="color: #008800; font-weight: bold">def</span> <span style="color: #0066BB; font-weight: bold">one_hot</span>(x):
  <span style="color: #008800; font-weight: bold">return</span> np<span style="color: #333333">.</span>identity(<span style="color: #0000DD; font-weight: bold">16</span>)[x:x<span style="color: #333333">+</span><span style="color: #0000DD; font-weight: bold">1</span>]

<span style="color: #888888"># ================================================================================</span>
init<span style="color: #333333">=</span>tf<span style="color: #333333">.</span>global_variables_initializer()

<span style="color: #888888"># ================================================================================</span>
<span style="color: #008800; font-weight: bold">with</span> tf<span style="color: #333333">.</span>Session() <span style="color: #008800; font-weight: bold">as</span> sess:
  sess<span style="color: #333333">.</span>run(init)

  <span style="color: #333333">================================================================================</span>
  <span style="color: #008800; font-weight: bold">for</span> i <span style="color: #000000; font-weight: bold">in</span> <span style="color: #007020">range</span>(num_episodes):
    s<span style="color: #333333">=</span>env<span style="color: #333333">.</span>reset() <span style="color: #888888"># Reset env per episode</span>
    e<span style="color: #333333">=</span><span style="color: #6600EE; font-weight: bold">1.</span><span style="color: #333333">/</span>((i<span style="color: #333333">/</span><span style="color: #0000DD; font-weight: bold">50</span>)<span style="color: #333333">+</span><span style="color: #0000DD; font-weight: bold">10</span>) <span style="color: #888888"># For exploration</span>
    rAll<span style="color: #333333">=</span><span style="color: #0000DD; font-weight: bold">0</span>
    done<span style="color: #333333">=</span><span style="color: #008800; font-weight: bold">False</span>
    local_loss<span style="color: #333333">=</span>[]

    <span style="color: #333333">================================================================================</span>
    <span style="color: #008800; font-weight: bold">while</span> <span style="color: #000000; font-weight: bold">not</span> done: <span style="color: #888888"># Q-network training</span>

      Qs<span style="color: #333333">=</span>sess<span style="color: #333333">.</span>run(Qpred,feed_dict<span style="color: #333333">=</span>{X:one_hot(s)})

      <span style="color: #008800; font-weight: bold">if</span> np<span style="color: #333333">.</span>random<span style="color: #333333">.</span>rand(<span style="color: #0000DD; font-weight: bold">1</span>)<span style="color: #333333">&lt;</span>e: <span style="color: #888888"># Exploration</span>
        a<span style="color: #333333">=</span>env<span style="color: #333333">.</span>action_space<span style="color: #333333">.</span>sample()
      <span style="color: #008800; font-weight: bold">else</span>:                   <span style="color: #888888"># Exploitation</span>
        a<span style="color: #333333">=</span>np<span style="color: #333333">.</span>argmax(Qs)

      <span style="color: #888888"># ================================================================================</span>
      <span style="color: #888888"># Execute action and get data from env</span>
      s1,reward,done,_<span style="color: #333333">=</span>env<span style="color: #333333">.</span>step(a)

      <span style="color: #333333">================================================================================</span>
      <span style="color: #008800; font-weight: bold">if</span> done: <span style="color: #888888"># If episode ended</span>
        <span style="color: #888888"># you update reward in Q value</span>
        Qs[<span style="color: #0000DD; font-weight: bold">0</span>,a]<span style="color: #333333">=</span>reward

      <span style="color: #008800; font-weight: bold">else</span>:    <span style="color: #888888"># If episode not ended</span>
        <span style="color: #888888"># Q value at next state</span>
        Qs1<span style="color: #333333">=</span>sess<span style="color: #333333">.</span>run(Qpred,feed_dict<span style="color: #333333">=</span>{X:one_hot(s1)})
        Qs[<span style="color: #0000DD; font-weight: bold">0</span>,a]<span style="color: #333333">=</span>reward<span style="color: #333333">+</span>dis<span style="color: #333333">*</span>np<span style="color: #333333">.</span>max(Qs1)

      <span style="color: #888888"># ================================================================================</span>
      <span style="color: #888888"># You train your network by using target Y and X (state)</span>
      sess<span style="color: #333333">.</span>run(train,feed_dict<span style="color: #333333">=</span>{X:one_hot(s),Y:Qs})

      <span style="color: #888888"># ================================================================================</span>
      rAll<span style="color: #333333">+=</span>reward
      s<span style="color: #333333">=</span>s1
    
    <span style="color: #888888"># ================================================================================</span>
    rList<span style="color: #333333">.</span>append(rAll)

<span style="color: #888888"># ================================================================================</span>
<span style="color: #007020">print</span>(<span style="background-color: #fff0f0">&quot;Percent of successful episodes: &quot;</span><span style="color: #333333">+</span><span style="color: #007020">str</span>(<span style="color: #007020">sum</span>(rList)<span style="color: #333333">/</span>num_episodes)<span style="color: #333333">+</span><span style="background-color: #fff0f0">&quot;%&quot;</span>)
plt<span style="color: #333333">.</span>bar(<span style="color: #007020">range</span>(<span style="color: #007020">len</span>(rList)),rList,color<span style="color: #333333">=</span><span style="background-color: #fff0f0">&quot;blue&quot;</span>)
plt<span style="color: #333333">.</span>show()
</pre></div>
<xmp>

================================================================================

</xmp>
   </BODY>
</HTML>
