<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;

 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;

    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;

    line-height:35px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"],
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
Lab_3_Dummy Q-learning (table)_03_0_q_table_frozenlake_det.py.html<br/>
basic q table
<xmp>
# https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0#.pjz9g59ap

import gym
import numpy as np
import matplotlib.pyplot as plt
from gym.envs.registration import register
import random as pr

# https://gist.github.com/stober/1943451

# This method is random argmax
def rargmax(vector):
    """ Argmax which randomly chooses among eligible maximum indices. """
    m = np.amax(vector)
    indices = np.nonzero(vector == m)[0]
    return pr.choice(indices)

register(
    id='FrozenLake-v3',
    entry_point='gym.envs.toy_text:FrozenLakeEnv',
    kwargs={'map_name': '4x4',
            'is_slippery': False}
)

env = gym.make('FrozenLake-v3')

# You initialize table with all zeros
# From $$$\hat{Q}(s,a)$$$
# We know s=16, a=4 as given value
# env has this values
Q = np.zeros([env.observation_space.n, env.action_space.n])

# You set number of iteration
num_episodes = 2000

# You create list to save results("total rewards" and "steps") per episode
rList = []
for i in range(num_episodes):
    # You initialize environment by reseting environment,
    # and get first new observation (state)
    state = env.reset()

    rAll = 0
    # We will interate episode until "done" becomes True
    done = False

    # The Q-Table learning algorithm
    while not done:
        # You will choose one action
        # In this case, you will choose action which outputs highest Q value from Q function
        # If values from Q function are same, 
        # you will randomly choose q value by using rargmax()
        # Fox example, if table has 0 in all area (for example, at very initial time),
        # you will move randomly
        # In this way, you choose one action

        # From state row (one state), select all column (all actions)
        # You select one action from all actions
        action = rargmax(Q[state, :])

        # You execute above chosen action
        # then, you get "new state", "reward" from environment
        new_state, reward, done, _ = env.step(action)

        # You will update Q-Table with new experience data with learning rate
        # Process of updating Q function (Q table) is most important job
        # Q value ,at current state, at doing current action, will be "reward" 
        # which you get from above process +
        # Q value at next step,
        # From new Q values, you will select highest Q value

        # You select new_state row (one state), 
        # from one state row, you select all actions
        # You extract max value from action values
        # You sum current reward and max value from action values
        # You store it
        # into current state row, current action column from q table,
        Q[state, action] = reward + np.max(Q[new_state, :])

        # You sum all rewards from each step
        rAll += reward
        state = new_state

    rList.append(rAll)

print("Success rate: " + str(sum(rList) / num_episodes))
print("Final Q-Table Values")
print("LEFT DOWN RIGHT UP")
print(Q)
plt.bar(range(len(rList)), rList, color="blue")
plt.show()
</xmp>
   </BODY>
</HTML>
