<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;

 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;

    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;

    line-height:35px;

    background-color: black;
    color:#ABBAB7
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"],
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
Lec_004: Q-learning (table) exploit&exploration and discounted reward.html
<xmp>
# https://www.youtube.com/watch?v=MQ-3QScrFSI&list=PLlMkM4tgfjnKsCWav-Z2F-MMFRx-2gMGG&index=6


# @
# The essence of q learning is updating current q funcion with following formular
# $$$\hat{Q}(s,a)\leftarrow r+max_{a'}\hat{Q}(s',a')$$$
# r : reward
# $$$max_{a'}\hat{Q}(s',a')$$$ : highest Q value at next state s'

# Exploit (you use current q value whic you know well) vs Exploration (you try to explore where you didn't go) dilemma
# img 2018-04-28 23-31-33.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/shkim-rl/pic/2018-04-28 23-31-33.png"><xmp>
# img 2018-04-28 23-33-44.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/shkim-rl/pic/2018-04-28 23-33-44.png"><xmp>

# @
# You can use algorithm called "e-greedy",
# to resolve exploit vs exploration dilemma

# You set small value
e=0.1

# If random value is smaller than e,
if rand<e:
    # you randomly choose action
    a=random
else:
    # you go along way you know as best way
    a=argmax(Q(s,a))

# If e=0.1, it means you randomly choose action by 10%,
# you go along best way by 90% which you know as best way

# But, going randomly becomes nonsense,
# as iteration goes high, 
# which means with tons of experinece there is no place to go randomly
# So, you will randomly go more often at initial training than experineced moment
# This way is called "decaying E-greedy"
for i in range(1000):
    e=0.1/(i+1)
    if random(1)<e:
        a=random
    else:
        a=argmax(Q(s,a))

# There are a lot of ways to implement "Exploit vs Exploration"
# Another way is adding random noise
# random_values: noise
# Without random_values, you will go best way you know
a=argmax(Q(s,a)+random_values)
a=argmax([0.5,0.6,0.3,0.2,0.5],[0.2,0.2,0.1,0.9,0.8])

# Adding random noise way is useful when you choose second, third best options

# @
# When you select "action",
# you will use "exploit&exploration" algorithm


# @
# "discounted reward"
# It's better reward you obtain currently than later

# Future reward is cheaper than current reward
# You will discount future rewards
# First, you define discount factor $$$\gamma$$$,
# which should be less than 1, greater than or equal to 0
# $$$\gamma=0.9$$$
# Then, you multiply it by next state's Q value

# $$$\hat{Q}(s,a) \leftarrow r + 0.9*max_{a'}\hat{Q}(s',a')$$$
# Above formular means you use current reward r,
# and you use next state's highest Q value which is discounted by 0.9
# So, you will move fast in direction you can get reward

# This is formular to calculate summed "reward" considered up to future reward
# $$$R=r_{1}+...+r_{n}$$$
# $$$R_{t}=r_{t}+r_{t+1}+...+r_{n}$$$

# We will talk about discounted rewards
# $$$r_{t}$$$: current reward
# $$$\gamma R_{t+1}$$$ discounted future reward
# $$$R_{t}=r_{t}+\gamma r_{t+1}+\gamma^{2}r_{t+2}+...+\gamma^{n-t}r_{n}$$$
# $$$R_{t}=r_{t}+\gamma (r_{t+1}+\gamma(r_{t+2}+...))$$$
# $$$R_{t}=r_{t}+\gamma R_{t+1}$$$

# Induced $$$R_{t}=r_{t}+\gamma R_{t+1}$$$ is similar with
# your essence Q learning algorithm $$$\hat{Q}(s,a) \leftarrow r + \gamma max_{a'}\hat{Q}(s',a')$$$
</xmp>
   </BODY>
</HTML>
