<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    line-height:35px;background-color: black;color:#ABBAB7;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"],
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
006_lec_Q-Network.html
<xmp>
# https://www.youtube.com/watch?v=w9GwqPx7LW8&list=PLlMkM4tgfjnKsCWav-Z2F-MMFRx-2gMGG&index=11

# @
# Q algorithm with q table is easy to use and shows good result
# But you can't apply Q algorithm with q table to real world question

# @
# To resolve this issue, you can use "q network",
# which takes 2 inputs (state s, action a)

# Then, you perform forward propagation,
# then, you obtain one output
# Or, you also can obtain multiple outputs from one input

# @
# This shows 2 cases
# One:
# Network takes in 2 inputs (state s, action a) as input data,
# then, network outputs one q value

# One:
# Network takes in 1 input (state s) as input data,
# then, network outputs multiple q values which can come from multiple actions
# img 2018-04-29 08-53-22.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/shkim-rl/pic/2018-04-29 08-53-22.png"><xmp>

# @
# You can set target function $$$y_{i}$$$ as following:
# $$$y_{i}=r_{j}$$$ when terminal of episode $$$\phi_{j+1}$$$
# $$$y_{i}=r_{j}+\gamma max_{a'}\;Q(\phi_{j+1},a';\theta)$$$ when non-terminal of episode $$$\phi_{j+1}$$$

# img 2018-04-29 09-44-40.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/shkim-rl/pic/2018-04-29 09-44-40.png"><xmp>

# @
# If you use q table, $$$\hat{Q}$$$ function converges to real Q function
# But if you use q network, $$$\hat{Q}$$$ function diverges from real Q function
# img 2018-04-29 09-45-30.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/shkim-rl/pic/2018-04-29 09-45-30.png"><xmp>

# @
# You can use reinforcement learning and neural network together
# But diverge issue sticks due to correlation between samples and non-stationary targets
# img 2018-04-29 09-46-23.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/shkim-rl/pic/2018-04-29 09-46-23.png"><xmp>

# @
# "Diverge" issue can be resolved by DQN architecture 
# (Deep, Replay experience, Separated networks) created by deepmind
# img 2018-04-29 09-47-39.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/shkim-rl/pic/2018-04-29 09-47-39.png"><xmp>

</xmp>
   </BODY>
</HTML>
