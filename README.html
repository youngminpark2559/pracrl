<br/><br/>
002_lec_Playing OpenAI GYM Games 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/002_lec_Playing OpenAI GYM Games.html">Go</a><br/>
002_lab_Playing OpenAI GYM Games 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/002_lab_Playing OpenAI GYM Games.html">Go</a><br/>
003_lab_Dummy Q-learning (table)_03_0_q_table_frozenlake_det.py 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_0_q_table_frozenlake_det.py.html">Go</a><br/>
003_lab_Dummy Q-learning (table)_03_1_q_table_frozenlake_det.py 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_1_q_table_frozenlake_det.py.html">Go</a><br/>
003_lab_Dummy Q-learning (table)_03_2_q_table_frozenlake_det.py 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_2_q_table_frozenlake_det.py.html">Go</a><br/>
004_lec_Q-learning (table) exploit&exploration and discounted reward 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/004_lec_Q-learning (table) exploit&exploration and discounted reward.html">Go</a><br/>
005_lec_Q-learning on Nondeterministic Worlds 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/005_lec_Q-learning on Nondeterministic Worlds.html">Go</a><br/>
005_001_lab_Q-learning on Nondeterministic Worlds___04_play_frozenlake.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/005_001_lab_Q-learning on Nondeterministic Worlds___04_play_frozenlake.py.html">Go</a><br/>
005_001_lab_Q-learning on Nondeterministic Worlds___05_0_q_table_frozenlake.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/005_001_lab_Q-learning on Nondeterministic Worlds___05_0_q_table_frozenlake.py.html">Go</a><br/>
005_001_lab_Q-learning on Nondeterministic Worlds___05_q_table_frozenlake.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/005_001_lab_Q-learning on Nondeterministic Worlds___05_q_table_frozenlake.py.html">Go</a><br/>
006_lec_Q-Network.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/006_lec_Q-Network.html">Go</a><br/>
006_001_lab_Q Network for Frozen Lake___06_q_net_frozenlake.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/006_001_lab_Q Network for Frozen Lake___06_q_net_frozenlake.py.html">Go</a><br/>
006_002_lab_Q Network for Cart Pole___07_0_random_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/006_002_lab_Q Network for Cart Pole___07_0_random_cartpole.py.html">Go</a><br/>
006_002_lab_Q Network for Cart Pole___07_1_q_net_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/006_002_lab_Q Network for Cart Pole___07_1_q_net_cartpole.py.html">Go</a><br/>
007_lec_DQN.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_lec_DQN.html">Go</a><br/>
007_000_lab_random_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_000_lab_random_cartpole.py.html">Go</a><br/>
007_001_lab_q_net_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_001_lab_q_net_cartpole.py.html">Go</a><br/>
007_002_lab_dqn_2013_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_002_lab_dqn_2013_cartpole.py.html">Go</a><br/>
007_003_lab_dqn_2015_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_003_lab_dqn_2015_cartpole.py.html">Go</a><br/>
007_001_lab_DQN 1 (NIPS 2013).html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_001_lab_DQN 1 (NIPS 2013).html">Go</a><br/>
007_002_lab_DQN 2 (Nature 2015).html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_002_lab_DQN 2 (Nature 2015).html">Go</a><br/>
010_001_Actor_Critic.ipynb.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/010_001_Actor_Critic.ipynb.html">Go</a><br/>

<br/>

Single lectures<br/>
dhkwak-Introduction of Deep Reinforcement Learning.html 
<a href="https://youngmtool.github.io/pracrl/dhkwak/dhkwak-Introduction of Deep Reinforcement Learning.html">Go</a><br/>
rlcode-understand a3c architecture 
<a href="https://youngmtool.github.io/pracrl/yt_single_lec/RLCode-A3C.py.html">Go</a><br/>

<br/>

awjuliani<br/>
q_learning_with_q_table.html 
<a href="https://youngmtool.github.io/pracrl/codes/q_learning_with_q_table.html">Go</a><br/>
q_learning_with_q_network.html 
<a href="https://youngmtool.github.io/pracrl/codes/q_learning_with_q_network.html">Go</a><br/>
q_learning_with_q_network_with_actions.html 
<a href="https://youngmtool.github.io/pracrl/codes/q_learning_with_q_network_with_actions.html">Go</a><br/>
q_learning_with_q_network_with_states_and_actions.html 
<a href="https://youngmtool.github.io/pracrl/codes/q_learning_with_q_network_with_states_and_actions.html">Go</a><br/>
q_learing_with_q_network_with_policy_gradient_in_cartpole_environment.html 
<a href="https://youngmtool.github.io/pracrl/codes/q_learing_with_q_network_with_policy_gradient_in_cartpole_environment.html">Go</a><br/>
build_model_nn_and_policy_reflecting_real_env_for_RL_question.html 
<a href="https://youngmtool.github.io/pracrl/codes/build_model_nn_and_policy_reflecting_real_env_for_RL_question.html">Go</a><br/>
Q_learing_with_DQN_plus_DoubleDQN_and_DuelingDQN_for_navigation_task.html 
<a href="https://youngmtool.github.io/pracrl/codes/Q_learing_with_DQN_plus_DoubleDQN_and_DuelingDQN_for_navigation_task.html">Go</a><br/>
deal_with_POMDP_question_with_Deep_Recurrent_Q_Network_along_with_Convolution_Layer.html 
<a href="https://youngmtool.github.io/pracrl/codes/deal_with_POMDP_question_with_Deep_Recurrent_Q_Network_along_with_Convolution_Layer.html">Go</a><br/>

<br/>

jwcleo<br/>
deal_with_cartpole_with_a2c.html 
<a href="https://youngmtool.github.io/pracrl/jwcleo/2-cartpole/2-actor-critic/deal_with_cartpole_with_a2c.html">Go</a><br/>

<br/>

rlcode<br/>
actor critic architecture to deal with cartpole environment 
<a href="https://github.com/youngmtool/pracrl/tree/master/rlcode/2-cartpole/2-actor-critic">Go</a><br/>
a2c architecture to deal with breakout atari game environment 
<a href="https://github.com/youngmtool/pracrl/rlcode/3-atari/1-breakout/breakout_a3c.py">Go</a><br/>



<br/><br/><br/><br/><br/>
