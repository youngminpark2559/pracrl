<br/><br/>

Single lectures<br/>
dhkwak-Introduction of Deep Reinforcement Learning.html 
<a href="https://youngmtool.github.io/pracrl/dhkwak/dhkwak-Introduction of Deep Reinforcement Learning.html">Go</a><br/>
rlcode-understand a3c architecture 
<a href="https://youngmtool.github.io/pracrl/yt_single_lec/RLCode-A3C.py.html">Go</a><br/>

<br/>

rlcode-implement reinforcement algorithms<br/>
actor critic architecture to deal with cartpole environment 
<a href="https://github.com/youngmtool/pracrl/blob/master/rlcode/2-cartpole/2-actor-critic/cartpole_a2c.py">Go</a><br/>
a3c architecture to deal with breakout atari game environment 
<a href="https://github.com/youngmtool/pracrl/blob/master/rlcode/3-atari/1-breakout/breakout_a3c.py">Go</a><br/>

<br/>

jwcleo-manually build environment, gui, agent<br/>
deal_with_cartpole_with_a2c.html 
<a href="https://youngmtool.github.io/pracrl/jwcleo/2-cartpole/2-actor-critic/deal_with_cartpole_with_a2c.html">Go</a><br/>

<br/>

single tutorial code<br/>
actor critic dealing with pendulum environment with keras 
<a href="https://github.com/youngmtool/pracrl/blob/master/single_tut_code/yash_patel/yash_patel-pendulum_by_actor_critic_with_keras_and_openai.py">yash_patel-pendulum_by_actor_critic_with_keras_and_openai.py</a><br/>

<br/>

awjuliani-Concepts of reinforcement learning and practical codes<br/>
q learning with q table 
<a href="https://github.com/youngmtool/pracrl/blob/master/codes/Part_000_001_Q_Learning_Agents.py">Part_000_001_Q_Learning_Agents.py</a><br/>
q learning with q network 
<a href="https://github.com/youngmtool/pracrl/blob/master/codes/Part_000_002_Q_Learning_Agents.py">Part_000_002_Q_Learning_Agents.py</a><br/>
q learning with q network which has one state and multiple actions 
<a href="https://github.com/youngmtool/pracrl/blob/master/codes/Part_001_Two_Armed_Bandit.py">Part_001_Two_Armed_Bandit.py</a><br/>
q learning with q network which has multiple states and multiple actions 
<a href="https://github.com/youngmtool/pracrl/blob/master/codes/Part_001.5_Contextual_Bandits.py">Part_001.5_Contextual_Bandits.py</a><br/>
q learing with q network where you use policy gradient based agent in cartpole question 
<a href="https://github.com/youngmtool/pracrl/blob/master/codes/Part_002_Policy_Based_Agents.py">Part_002_Policy_Based_Agents.py</a><br/>
build_model_nn_and_policy_reflecting_real_env_for_RL_question.html 
<a href="https://youngmtool.github.io/pracrl/codes/build_model_nn_and_policy_reflecting_real_env_for_RL_question.html">Go</a><br/>
Q_learing_with_DQN_plus_DoubleDQN_and_DuelingDQN_for_navigation_task.html 
<a href="https://youngmtool.github.io/pracrl/codes/Q_learing_with_DQN_plus_DoubleDQN_and_DuelingDQN_for_navigation_task.html">Go</a><br/>
deal with "partial obserbability markov decision process" question with deep recurrent q network and convolution layer 
<a href="https://github.com/youngmtool/pracrl/blob/master/codes/Part_006_Partial_Observability_and_Deep_Recurrent_Q_Networks.py">Part_006_Partial_Observability_and_Deep_Recurrent_Q_Networks.py</a><br/>

<br/>

shkim-Concepts of reinforcement learning and practical codes<br/>
002_lec_Playing OpenAI GYM Games 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/002_lec_Playing OpenAI GYM Games.html">Go</a><br/>
002_lab_Playing OpenAI GYM Games 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/002_lab_Playing OpenAI GYM Games.html">Go</a><br/>
003_lab_Dummy Q-learning (table)_03_0_q_table_frozenlake_det.py 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_0_q_table_frozenlake_det.py.html">Go</a><br/>
003_lab_Dummy Q-learning (table)_03_1_q_table_frozenlake_det.py 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_1_q_table_frozenlake_det.py.html">Go</a><br/>
003_lab_Dummy Q-learning (table)_03_2_q_table_frozenlake_det.py 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_2_q_table_frozenlake_det.py.html">Go</a><br/>
004_lec_Q-learning (table) exploit&exploration and discounted reward 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/004_lec_Q-learning (table) exploit&exploration and discounted reward.html">Go</a><br/>
005_lec_Q-learning on Nondeterministic Worlds 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/005_lec_Q-learning on Nondeterministic Worlds.html">Go</a><br/>
005_001_lab_Q-learning on Nondeterministic Worlds___04_play_frozenlake.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/005_001_lab_Q-learning on Nondeterministic Worlds___04_play_frozenlake.py.html">Go</a><br/>
005_001_lab_Q-learning on Nondeterministic Worlds___05_0_q_table_frozenlake.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/005_001_lab_Q-learning on Nondeterministic Worlds___05_0_q_table_frozenlake.py.html">Go</a><br/>
005_001_lab_Q-learning on Nondeterministic Worlds___05_q_table_frozenlake.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/005_001_lab_Q-learning on Nondeterministic Worlds___05_q_table_frozenlake.py.html">Go</a><br/>
006_lec_Q-Network.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/006_lec_Q-Network.html">Go</a><br/>
006_001_lab_Q Network for Frozen Lake___06_q_net_frozenlake.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/006_001_lab_Q Network for Frozen Lake___06_q_net_frozenlake.py.html">Go</a><br/>
006_002_lab_Q Network for Cart Pole___07_0_random_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/006_002_lab_Q Network for Cart Pole___07_0_random_cartpole.py.html">Go</a><br/>
006_002_lab_Q Network for Cart Pole___07_1_q_net_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/006_002_lab_Q Network for Cart Pole___07_1_q_net_cartpole.py.html">Go</a><br/>
007_lec_DQN.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_lec_DQN.html">Go</a><br/>
007_000_lab_random_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_000_lab_random_cartpole.py.html">Go</a><br/>
007_001_lab_q_net_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_001_lab_q_net_cartpole.py.html">Go</a><br/>
007_002_lab_dqn_2013_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_002_lab_dqn_2013_cartpole.py.html">Go</a><br/>
007_003_lab_dqn_2015_cartpole.py.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_003_lab_dqn_2015_cartpole.py.html">Go</a><br/>
007_001_lab_DQN 1 (NIPS 2013).html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_001_lab_DQN 1 (NIPS 2013).html">Go</a><br/>
007_002_lab_DQN 2 (Nature 2015).html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/007_002_lab_DQN 2 (Nature 2015).html">Go</a><br/>
010_001_Actor_Critic.ipynb.html 
<a href="https://youngmtool.github.io/pracrl/shkim-rl/010_001_Actor_Critic.ipynb.html">Go</a><br/>

<br/>



<br/><br/><br/><br/><br/>
