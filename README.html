<br/><br/>

<h2>Single lectures</h2>
<a href="https://youngminpark2559.github.io/pracrl/dhkwak/dhkwak-Introduction of Deep Reinforcement Learning.html">dhkwak-Introduction of Deep Reinforcement Learning</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/yt_single_lec/RLCode-A3C.py.html">rlcode-understand a3c architecture</a><br/>

<br/>
<h2>rlcode-implement reinforcement algorithms</h2>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/rlcode/2-cartpole/2-actor-critic/cartpole_a2c.py">actor critic architecture to deal with cartpole environment</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/rlcode/3-atari/1-breakout/breakout_a3c.py">a3c architecture to deal with breakout atari game environment</a><br/>

<br/>
<h2>jwcleo-manually build environment, gui, agent</h2>
<a href="https://youngminpark2559.github.io/pracrl/jwcleo/2-cartpole/2-actor-critic/deal_with_cartpole_with_a2c.html">deal_with_cartpole_with_a2c</a><br/>

<br/>
<h2>single tutorial code</h2>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/single_tut_code/yash_patel/yash_patel-pendulum_by_actor_critic_with_keras_and_openai.py">actor critic dealing with pendulum environment with keras</a><br/>

<br/>
<h2>awjuliani-Concepts of reinforcement learning and practical codes</h2>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_000_001_Q_Learning_Agents.py">q learning with q table</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_000_002_Q_Learning_Agents.py">q learning with q network</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_001_Two_Armed_Bandit.py">q learning with q network which has one state and multiple actions</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_001.5_Contextual_Bandits.py">q learning with q network which has multiple states and multiple actions</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_002_Policy_Based_Agents.py">q learning with q network where you use policy gradient based agent in cartpole question</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_003_Model_Based_RL.py">build model network and policy network reflecting real environment for RL question</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_004_Deep_Q_Networks_and_Beyond.py">q learing with DoubleDQN and DuelingDQN for navigation task</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_006_Partial_Observability_and_Deep_Recurrent_Q_Networks.py">deal with "partial obserbability markov decision process" question with deep recurrent q network and convolution layer</a><br/>

<br/>
<h2>shkim-Concepts of reinforcement learning and practical codes</h2>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/002_lec_Playing OpenAI GYM Games.html">002_lec_Playing OpenAI GYM Games </a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/002_lab_Playing OpenAI GYM Games.html">002_lab_Playing OpenAI GYM Games</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_0_q_table_frozenlake_det.py.html">003_lab_Dummy Q-learning (table)_03_0_q_table_frozenlake_det</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_1_q_table_frozenlake_det.py.html">003_lab_Dummy Q-learning (table)_03_1_q_table_frozenlake_det</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/003_lab_Dummy Q-learning (table)_03_2_q_table_frozenlake_det.py.html">003_lab_Dummy Q-learning (table)_03_2_q_table_frozenlake_det</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/004_lec_Q-learning (table) exploit&exploration and discounted reward.html">004_lec_Q-learning (table) exploit&exploration and discounted reward</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/005_lec_Q-learning_on_Nondeterministic_Worlds.html">005_lec_Q-learning_on_Nondeterministic_Worlds</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/shkim-rl/005_001_lab_Q-learning on Nondeterministic Worlds___04_play_frozenlake.py">005_001_lab_Q-learning on Nondeterministic Worlds___04_play_frozenlake</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/shkim-rl/005_001_lab_Q-learning_on_Nondeterministic_Worlds___05_0_q_table_frozenlake.py">005_001_lab_Q-learning_on_Nondeterministic_Worlds___05_0_q_table_frozenlake</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/shkim-rl/005_001_lab_Q-learning_on_Nondeterministic_Worlds___05_q_table_frozenlake.py">005_001_lab_Q-learning_on_Nondeterministic_Worlds___05_q_table_frozenlake</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/006_lec_Q-Network.html">006_lec_Q-Network</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/shkim-rl/006_001_lab_Q_Network_for_Frozen_Lake___06_q_net_frozenlake.py">006_001_lab_Q_Network_for_Frozen_Lake___06_q_net_frozenlake</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/006_002_lab_Q Network for Cart Pole___07_0_random_cartpole.py.html">006_002_lab_Q Network for Cart Pole___07_0_random_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/006_002_lab_Q Network for Cart Pole___07_1_q_net_cartpole.py.html">006_002_lab_Q Network for Cart Pole___07_1_q_net_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_lec_DQN.html">007_lec_DQN</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_000_lab_random_cartpole.py.html">007_000_lab_random_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_001_lab_q_net_cartpole.py.html">007_001_lab_q_net_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_002_lab_dqn_2013_cartpole.py.html">007_002_lab_dqn_2013_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_003_lab_dqn_2015_cartpole.py.html">007_003_lab_dqn_2015_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_001_lab_DQN 1 (NIPS 2013).html">007_001_lab_DQN 1 (NIPS 2013)</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_002_lab_DQN 2 (Nature 2015).html">007_002_lab_DQN 2 (Nature 2015)</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/010_001_Actor_Critic.ipynb.html">010_001_Actor_Critic</a><br/>

<br/>
<h2>PangyoLab - Reinforcement learning theory</h2>
<a href="https://youngminpark2559.github.io/pracrl/pangyolab/rl_theory/003.html">Planning by using dynamic programming</a><br/>

<br/>
<h2>PangyoLab - Review AlphaGo paper</h2>
<a href="https://youngminpark2559.github.io/pracrl/pangyolab/alphago_paper_review/001.html">001 Monte Carlo Tree Search</a><br/>

<br/><br/><br/><br/><br/><br/>
