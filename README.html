<br/><br/>

<h2>Single lectures</h2>
<a href="https://youngminpark2559.github.io/pracrl/dhkwak/dhkwak-Introduction of Deep Reinforcement Learning.html">dhkwak-Introduction of Deep Reinforcement Learning</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/yt_single_lec/RLCode-A3C.py.html">rlcode-understand a3c architecture</a><br/>

<br/>
<h2>rlcode-implement reinforcement algorithms</h2>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/rlcode/2-cartpole/2-actor-critic/cartpole_a2c.py">actor critic architecture to deal with cartpole environment</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/rlcode/3-atari/1-breakout/breakout_a3c.py">a3c architecture to deal with breakout atari game environment</a><br/>

<br/>
<h2>jwcleo-manually build environment, gui, agent</h2>
<a href="https://youngminpark2559.github.io/pracrl/jwcleo/2-cartpole/2-actor-critic/deal_with_cartpole_with_a2c.html">deal_with_cartpole_with_a2c</a><br/>

<br/>
<h2>single tutorial code</h2>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/single_tut_code/yash_patel/yash_patel-pendulum_by_actor_critic_with_keras_and_openai.py">actor critic dealing with pendulum environment with keras</a><br/>

<br/>
<h2>awjuliani-Concepts of reinforcement learning and practical codes</h2>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_000_001_Q_Learning_Agents.py">q learning with q table</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_000_002_Q_Learning_Agents.py">q learning with q network</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_001_Two_Armed_Bandit.py">q learning with q network which has one state and multiple actions</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_001.5_Contextual_Bandits.py">q learning with q network which has multiple states and multiple actions</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_002_Policy_Based_Agents.py">q learning with q network where you use policy gradient based agent in cartpole question</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_003_Model_Based_RL.py">build model network and policy network reflecting real environment for RL question</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_004_Deep_Q_Networks_and_Beyond.py">q learing with DoubleDQN and DuelingDQN for navigation task</a><br/>
<a href="https://github.com/youngminpark2559/pracrl/blob/master/codes/Part_006_Partial_Observability_and_Deep_Recurrent_Q_Networks.py">deal with "partial obserbability markov decision process" question with deep recurrent q network and convolution layer</a><br/>

<br/>
<h2>SHKim-Reinforcement learning and practical codes</h2>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/002_Playing_OpenAI_GYM_Games/001_Lec/001/main.html">Lec_002_Playing_OpenAI_GYM_Games</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/002_Playing_OpenAI_GYM_Games/002_Lab/001/main.html">Lab_002_Playing_OpenAI_GYM_Games</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/003_Dummy_Q_learning_using_table/001_Lec/001/main.html">Lec_003_Dummy_Q_learning_using_table</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/003_Dummy_Q_learning_using_table/002_Lab/001/main.html">Lab_003_Dummy_Q_learning_using_table</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/004_Q-learning_using_table_Exploit_and_exploration_Discounted_reward/001_Lec/001/main.html">Lec_004_Q-learning_using_table_Exploit_and_exploration_Discounted_reward</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/004_Q-learning_using_table_Exploit_and_exploration_Discounted_reward/002_Lab/001_Use_adding_noise_to_Q_values/main.html">Lab_004_Q-learning_using_table_Exploit_and_exploration_Discounted_reward_using_adding_random_noise_to_Q_values</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/004_Q-learning_using_table_Exploit_and_exploration_Discounted_reward/002_Lab/002_Use_e_greedy/main.html">Lab_004_Q-learning_using_table_Exploit_and_exploration_Discounted_reward_using_adding_e_greedy</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/005_Q-learning_on_nondeterministic_world/001_Lec/main.html">Lec_005_Q-learning_on_nondeterministic_worldy</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/005_Q-learning_on_nondeterministic_world/002_Lab/001_Play_FrozenLake_game_on_non_deterministic_environement/main.html">Lab_005_001_Play_FrozenLake_game_on_non_deterministic_environement</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/005_Q-learning_on_nondeterministic_world/002_Lab/002_Use_Q_mentor_on_non_deterministic_environement/main.html">Lab_005_002_Use_Q_mentor_on_non_deterministic_environement</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/005_Q-learning_on_nondeterministic_world/002_Lab/003_Use_Q_mentor_with_learning_rate_on_non_deterministic_environement/main.html">Lab_005_003_Use_Q_mentor_with_learning_rate_on_non_deterministic_environement</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/006_Q-network/001_Lec/001/main.html">Lec_006_Q-network</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/006_Q-network/002_Lab/001/main.html">Lab_006_001_Q-network_grid_world</a><br/>

<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_lec_DQN.html">007_lec_DQN</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_000_lab_random_cartpole.py.html">007_000_lab_random_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_001_lab_q_net_cartpole.py.html">007_001_lab_q_net_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_002_lab_dqn_2013_cartpole.py.html">007_002_lab_dqn_2013_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_003_lab_dqn_2015_cartpole.py.html">007_003_lab_dqn_2015_cartpole</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_001_lab_DQN 1 (NIPS 2013).html">007_001_lab_DQN 1 (NIPS 2013)</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/007_002_lab_DQN 2 (Nature 2015).html">007_002_lab_DQN 2 (Nature 2015)</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/shkim-rl/010_001_Actor_Critic.ipynb.html">010_001_Actor_Critic</a><br/>

<br/>
<h2>PangYoLab - Reinforcement learning theory</h2>
<a href="https://youngminpark2559.github.io/pracrl/pangyolab/rl_theory/001_Introduction_to_RL/main.html">001_Introduction_to_RL</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/pangyolab/rl_theory/002_Markov_decision_process/main.html">002_Markov_decision_process</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/pangyolab/rl_theory/003_Planning_by_dynamic_programming/003.html">003_Planning_by_dynamic_programming</a><br/>

<br/>
<h2>PangYoLab - Review AlphaGo paper</h2>
<a href="https://youngminpark2559.github.io/pracrl/pangyolab/alphago_paper_review/001/main.html">001 Monte Carlo Tree Search</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/pangyolab/alphago_paper_review/002/main.html">002</a><br/>
<a href="https://youngminpark2559.github.io/pracrl/pangyolab/alphago_paper_review/003/main.html">003</a><br/>

<br/><br/><br/><br/><br/><br/>
