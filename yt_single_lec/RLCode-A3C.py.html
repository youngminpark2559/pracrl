<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN"
   "http://www.w3.org/TR/html4/strict.dtd">
<HTML>
   <HEAD>
      <TITLE>My first HTML document</TITLE>
      <style rel="stylesheet" type="text/css">
body {
 font-size: 23px;
 margin-top: 50px;
    margin-bottom: 50px;
    margin-right: 80px;
    margin-left: 80px;
    padding-top: 50px;
    padding-bottom: 50px;
    padding-right: 80px;
    padding-left: 80px;
    line-height:35px;
}
</style>
      <script type="text/x-mathjax-config">
MathJax.Hub.Config({
    "HTML-CSS" : {
        availableFonts : ["STIX"],
        preferredFont : "STIX",
        webFont : "STIX-Web",
        imageFont : null
    }
});
</script>
     <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js" type="text/javascript">
    MathJax.Hub.Config({
        HTML: ["input/TeX","output/HTML-CSS"],
        TeX: { extensions: ["AMSmath.js","AMSsymbols.js"],
               equationNumbers: { autoNumber: "AMS" } },
        extensions: ["tex2jax.js"],
        jax: ["input/TeX","output/HTML-CSS"],
        tex2jax: { inlineMath: [ ['$$$','$$$'] ],
                   displayMath: [ ['$$$$','$$$$'] ],
                   processEscapes: true },
        "HTML-CSS": { availableFonts: ["TeX"],
                      linebreaks: { automatic: true } }
    });
</script>
   </HEAD>
   <BODY>
RLCode-A3C.py.html
<xmp>
# RLCode와 A3C 쉽고 깊게 이해하기
# https://www.youtube.com/watch?v=gINks-YCTBs&t=621s

# RLCode-A3C.py.html

# @
# A3C uses policy based algorithm
# A3C resolves correlation sample issue by asynchronous update
# so, it doesn't need to use replay memory
# so, it uses policy gradient algorithm (which is called actor-critic architecture)
# A3C shows fast training speed 
# because multiple agents interact with environment at the same time

# A3C is architecture which asynchronously updates actor-critic architecture,
# which means A3C create multiple actor-critic agents and environments

# Actor-critic is architecture 
# which trains REINFORCE algorithm in online way (in other words, real time training)

# REINFORCE algorithm is algorithm 
# which updates Monte-Carlo by using policy gradient

# Monte-Carlo means, when there are episodes, 
# Monte-Carlo updates itself every each episode


# @
# There is cross entropy loss function which is used for supervised learning
# Cross entropy loss function can be notated as following:
# $$$-\sum y_{i}\log{p_{i}}$$$ which tells you how different between p and y
# img 2018-05-06 16-45-16.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 16-45-16.png"><xmp>

# img 2018-05-06 16-46-16.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 16-46-16.png"><xmp>

# @
# You can use cross entropy loss function for reinforcement learning
# Let's say you have policy
# And you can approximate that policy into neural network
# That approximated network takes in each state vector as input data
# And that approximated network outputs probability of each action which agent can execute
# And you can consider probability as prediction p
# And there should be action which agent actually execute,
# and you can consider it as label y
# img 2018-05-06 16-49-56.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 16-49-56.png"><xmp>

# Then, you can use cross entropy loss function on it
# States as input data: $$$s_{0},s_{1},s_{2},...$$$
# We input that input data into network appximated from policy
# Network outputs prediction p representing probability
# Actual action which agent executes : $$$a_{0},a_{1},a_{2},...$$$

# But it's odd you see actual action as label

# So, you need direction for update
# img 2018-05-06 16-50-35.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 16-50-35.png"><xmp>

# @
# You know q function which represents how much good actural action is
# $$$Q_{\pi}(s,a)=E_{\pi}[R_{t+1}+\gamma R_{t+2}+...|S_{t}=s,A_{t}=a]$$$

# You can use q function as direction and size for update
# And you can say q function in this circumstance as critic
# Critic also can be approximated to neural network 
# just like policy is approximated to neural network
# In other words, critic uses neural network to know q function

# And then you can apply gradient descent 
# on function from $$$Q(S_{t},A_{t})(-\sum y_{i}\log{p_{i}})$$$
# img 2018-05-06 16-59-40.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 16-59-40.png"><xmp>

# Actor critic is one agent
# Using only one actor critic is slow performance
# And it can't resolve correlation sample issue

# To resolve above issue, you can use multiple actor critic agents
# Generally, people create 16 agents
# img 2018-05-06 17-03-59.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 17-03-59.png"><xmp>

# @
# Agent1 asynchronously updates global network
# Other agents keep going tasks
# Then, finally global network updates agent1
# img 2018-05-06 17-05-49.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 17-05-49.png"><xmp>


# Agent1 updates global network
# img 2018-05-06 17-06-37.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 17-06-37.png"><xmp>

# Global network updates agent1
# img 2018-05-06 17-07-18.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 17-07-18.png"><xmp>


# @
# Goal of reinforcement learning increases probability of action which gives most reward

# Policy means rules which agent use when selecting action at each state

# Agent obtains experience data from environment,
# then it(containing policy) updates itself

# It means you need 1.criterion for update policy
# You also need 2.way of update policy based on that criterion

# Criterion will be target function which should be learned by agent
# Way will be gradient ascent applied on target function to update policy

# @
# First, you should consider approximating policy
# If state of question which agent wants to learn is huge and high dimension,
# it's almost impossible for agent to have policies on each state
# So, you need to consider states as input value of function f,
# then you approximate policy into function f.
# It means output of function f is action

# img 2018-05-06 17-59-48.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 17-59-48.png"><xmp>

# @
# Each arrow is policy at each state
# So you will use x y coordinates representing state as input data
# You will use approximated policy function $$$\pi$$$
# Then, policy function outputs action out of up,down,right,left
# img 2018-05-06 18-03-11.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 18-03-11.png"><xmp>

# @
# There can be various ways to approximate policy
# You will approximate policy into policy function by using deep learning network
# In other words, policy function will be layers in neural network
# What you should do is to define input and output

# @
# $$$\pi(a|s)=P[A_{t}=a|S_{t}=s]$$$: policy $$$\pi$$$ means probability of occurring a, at given state s

# $$$\pi_\theta(a|s)=P[A_{t}=a|S_{t}=s,\theta]$$$: you write this way 
# since you approximate policy $$$\pi$$$ by neural network $$$\theta$$$

# Since you're using neural network, you would intialize neural network
# Goal you do is update that neural network

# img 2018-05-06 18-11-19.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 18-11-19.png"><xmp>

# @
# Now, you approximated policy into target function by using neural network
# Then, you need to apply gradient ascent on target function

# <
# @
# In RL, there are 2 streams (value based reinforcement learnig, policy based reinforcement learnig)

# Value based reinforcement learnig:
# Criterion which agent uses to select action at every step is value function

# Policy based reinforcement learnig
# Criterion which agent uses to select action at every update is target function $$$J(\theta)$$$
# $$$\theta$$$ is neural network which approximated policy
# img 2018-05-06 18-18-13.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 18-18-13.png"><xmp>

# @
# Let's think about pathway 
# which agent will walk along based on policy $$$\pi_{\theta}$$$

# And you will call that pathway as trajectory $$$\tau$$$,
# which represents trace of interation between agent and environment

# $$$s_{0}$$$: initial state
# $$$a_{0}$$$: initail chosen action basend on policy
# $$$r_{1}$$$: reward
# Entire trajectory $$$\tau=s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...,s_{T}$$$

# J(\theta) means sum of reward during finishing trajectory
# J(\theta) changes everytime because trajectory changes everytime
# So, you can use technique of expectation to resolve above trouble
# $$$J(\theta)=E[\sum\limits_{t=0}^{T-1}r_{t+1}|\pi_{\theta}] $$$
# $$$J(\theta)=E[r_{1}+r_{2}+r_{3}+...+r_{T}|\pi_{\theta}]$$$
# $$$J(\theta)$$$ is target function which agent should increase

# @
# You will update neural network $$$\theta$$$, 
# by using gradient ascent on target function $$$J(\theta)$$$
# $$$\theta'=\theta+\alpha \nabla_{\theta}J(\theta)$$$
# $$$\theta'$$$ is new updated policy
# $$$\nabla_{\theta}J(\theta)$$$ is policy gradient
# In other words, policy gradient is differentiation of target function J(\theta) 
# in respect to network $$$\theta$$$

# @
# Basic algorithm doing policy gradient is REINFORCE
# REINFORCE can be notated as following
# $$$\nabla_{\theta}J(\theta)\sim \sum\limits_{t=0}^{T-1} \nabla_{\theta}\log{\pi_{\theta}}(a_{t}|s_{t})G_{t}$$$

# Again, target function $$$J(\theta)$$$ for policy gradient can be also notated as following
# $$$J(\theta)=E[\sum\limits_{t=0}^{T-1}r_{t+1}|\pi_{\theta}]$$$
# Agent should increate above target function by using gradient ascent

# Expectation value = sum of (probability of occurring x)*(value at x) in respect to x
# For example, expectation value from dice
# $$$E[f(x)]=\sum\limits_{x}p(x)f(x)$$$

# When trajectory $$$\tau = s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...,s_{T}$$$
# target function $$$J(\theta)$$$ can be notated as following
# $$$J(\theta)=E[\sum\limits_{t=1}^{T-1}r_{t+1}|\pi_{\theta}]$$$
# $$$J(\theta)=E_{\tau}[r_{1}|\pi_{\theta}] + E_{\tau}[r_{2}|\pi_{\theta}] + E_{\tau}[r_{3}|\pi_{\theta}] + ...$$$
# $$$E_{\tau}[r_{1}|\pi_{\theta}]$$$ : expectation value in respect to $$$r_{1}$$$ at tragectory $$$\tau$$$
# $$$J(\theta)=\sum\limits_{t=0}^{T-1}P(s_{t},a_{t}|\tau)R(s_{t},a_{t})$$$
# $$$P(s_{t},a_{t}|\tau)$$$ : probability at $$$(s_{t},a_{t})$$$
# $$$R(s_{t},a_{t})$$$ : value at $$$(s_{t},a_{t})$$$
# $$$J(\theta)=\sum\limits_{t=0}^{T-1}P(s_{t},a_{t}|\tau)r_{t+1}$$$

# Perform $$$\nabla_{\theta}$$$ (differentiation) on both side
# $$$\nabla_{\theta}J(\theta)=\nabla_{\theta} E[\sum\limits_{t=1}^{T-1}r_{t+1}|\pi_{\theta}]$$$
# $$$\nabla_{\theta}J(\theta)=\nabla_{\theta} \sum\limits_{t=0}^{T-1} P(s_{t},a_{t}|\tau)r_{t+1}$$$
# $$$\nabla_{\theta}J(\theta)=\sum\limits_{t=0}^{T-1} \nabla_{\theta} P(s_{t},a_{t}|\tau)r_{t+1}$$$
# We will inspect this step more detail
# $$$\nabla_{\theta}J(\theta)=\sum\limits_{t=0}^{T-1} P(s_{t},a_{t}|\tau) \nabla_{\theta} \log{P(s_{t},a_{t}|\tau)r_{t+1}} $$$


# Step:
# $$$\nabla_{\theta}J(\theta)=\sum\limits_{t=0}^{T-1} \nabla_{\theta} P(s_{t},a_{t}|\tau)r_{t+1}$$$
# According to characteristic of log differentiation
# $$$\frac{d}{dx}\log{x}=\frac{1}{x}$$$
# $$$\frac{d}{dx}\log{f(x)}=\frac{\frac{d}{dx}f(x)}{f(x)}$$$

# $$$\nabla_{\theta}J(\theta) = \sum\limits_{t=0}^{T-1} P(s_{t},a_{t}|\tau) \frac{\nabla_{\theta} P(s_{t},a_{t}|\tau)}{P(s_{t},a_{t}|\tau)} r_{t+1}$$$
# Finally,
# $$$\nabla_{\theta}J(\theta) = \sum\limits_{t=0}^{T-1} P(s_{t},a_{t}|\tau) \nabla_{\theta} \log{P(s_{t},a_{t}|\tau)r_{t+1}}$$$
# You convert above formular into expectation value notation
# $$$\nabla_{\theta}J(\theta) = E_{\tau}[\sum\limits_{t=0}^{T-1} \nabla_{\theta} \log{P(s_{t},a_{t}|\tau)r_{t+1}}]$$$
# When it comes to reinforcement learing,
# you can't calculate E[],
# so you do sampling,
# which means you do one try and calculate and the update,
# you do second try and calculate and the update,...
# $$$\rightarrow \sum\limits_{t=0}^{T-1}\nabla_{\theta}\log{P(s_{t},a_{t}|\tau)r_{t+1}}$$$
# You should find $$$\log{P(s_{t},a_{t}|\tau)}$$$ to go further

# $$$P(s_{t},a_{t}|\tau)=P(s_{0},a_{0},r_{1},s_{1},a_{1},r_{2},...,s_{t},a_{t}|\theta])$$$
# You will see traces of interaction between agent and environment from following notation
# $$$P(s_{t},a_{t}|\tau)=P(s_{0})\pi_{\theta}(a_{0}|s_{0}) P(s_{1}|s_{0},a_{0})\pi_{\theta}(a_{1}|s_{1}) P(s_{2}|s_{1},a_{1})...$$$
# P is environment
# $$$\pi$$$ is agent
# Agent can't know P

# We can try to perform log on both side
# $$$\log{P(s_{t},a_{t}|\tau)}=\log{} P(s_{0})\pi_{\theta}(a_{0}|s_{0}) P(s_{1}|s_{0},a_{0})\pi_{\theta}(a_{1}|s_{1}) ... P(s_{t}|s_{t-1},a_{t-1})\pi_{\theta}(a_{t}|s_{t})$$$
# We can use following rule,
# $$$\log{AB}=\log{A}+\log{B}$$$
# $$$\log{P(s_{t},a_{t}|\tau)}=\log{P(s_{0})} + \log{\pi_{\theta}(a_{0}|s_{0})} +  \log{P(s_{1}|s_{0},a_{0})} + \log{\pi_{\theta}(a_{1}|s_{1})} + ... + \log{P(s_{t}|s_{t-1},a_{t-1})} + \log{\pi_{\theta}(a_{t}|s_{t})}$$$
# ...
# To do writing formulars

# @
# REINFORCE
# 1. You run one episode based on current policy
# 1. You write trajectory which is obtained from each running
# 1. When episode ends, you calculate $$$G_{t}$$$
# If written trajectory values are 100, it means you obtain 100 returns
# 1. You calculate policy gradient, then update policy
# 1. You iterate above 1-4 steps

# Since you update policy per episode,
# you call this way as Monte Carlo policy gradient, or REINFORCE
# Updating policy per "episode" doesn't mean "online"

# @
# Issue of REINFORCE
# 1. It creates high variance 
# because as episode becomes longer, you can obtain various trajectory
# 1. You only can update policy per episode (can't be with online)

# Resolving ways:
# 1. You can upgrade from Monte Carlo to TD (Temporal Difference)
# Temporal Difference means you update policy more often than Monte Carlo,
# Monte Carlo means you update when episode is finished
# 1. You can upgrade from Monte Carlo to Actor-Critic
# Actor-Critic means you update policy per each step

# @
# Simply, actor-critic means you use 2 networks
# One (actor network) will be used as policy
# One (critic network) will be used as q function

# Critic (q function) can be considered as critic person 
# who criticises agent's action with saying that actions is either good or bad

# Then, you can update policy per every step

# @
# You won't use q function as it is.
# For example, suppose moving to right is +2 which means good action.
# Suposse moving to left is +1 which means bad action.

# But if we think right is +1, left is -1,
# task of update and train will be easier

# Therefore, you will use advantage function instead of q function.
# Advantage function is generated by (q function) - (baseline which means average reward)
# By substacting them, it reduces variance

# @
# Q function means value from at specific state, by specific action
# Value function is baseline,
# value function is value from at specific state, by general action


# @
# Approximating Q and V is inefficient.

# @
# Actor approximates policy to $$$\theta$$$
# You update actor by using actor's loss function 
# $$$\nabla_{theta}\log{\pi_{\theta}}(a_{t}|s_{t})(r_{t+1}+\gamma V_{v}(s_{t+1}-V_{v}(s_{t}))$$$

# Critic approximates value function to v
# You update critic by using MSE loss function 
# $$$ (r_{t+1}+\gamma V_{v}(s_{t+1})-V_{v}(s_{t}))^{2} $$$

# @
# Actor network (policy network) outputs policy
# Critic network (value network) outputs value
# You can update actor network (policy network) by using (cross entropy)*(TD error)
# You can update critic network (value network) by using TD error
# After update, you proceed one step, then you update,
# which means you update per step unlike REINFORCE,
# which should reach end of episode to update

# Actor network (value network) calculates value,
# representing how much good action at each step is
# img 2018-05-06 19-53-53.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 19-53-53.png"><xmp>

# Illustraion between actor and critic
# img 2018-05-06 19-54-49.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 19-54-49.png"><xmp>

# @
# A3C:
# In process of update,
# actor critic processes update like one step, one update,
# However, a3c goes 5 steps or 10 steps or anything then asynchronously updates

# If you go 20 steps, after 20 steps, you obtain one loss function
# Until 19 steps, you add actual reward you obtain, at 20 step, you use value function,
# which means you create multi steps
# Since you put more step information, precision will increase, with decreasing bias

# Illustraion of overview step on A3C
# img 2018-05-06 20-08-26.png
# </xmp><img src="https://raw.githubusercontent.com/youngmtool/pracrl/master/yt_single_lec/pic/2018-05-06 20-08-26.png"><xmp>
</xmp>
   </BODY>
</HTML>
